{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2108fe6f-2fcb-40da-ae71-4d2013cfb0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb5bed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57819988",
   "metadata": {},
   "source": [
    "## 1. Global parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631cb1e",
   "metadata": {},
   "source": [
    "## 2. Building blocks functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1997641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt_col(X , column='week_start_date', format='%Y-%m-%d'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X\n",
    "        col = weekofx\n",
    "\n",
    "    Returns:\n",
    "        X with X[col].dtype = datetime\n",
    "\n",
    "    \"\"\"\n",
    "    X_new = X.copy()\n",
    "    X_new[column] = pd.to_datetime(X_new[column], format=format)\n",
    "\n",
    "    return X_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "69948f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(features_dir, labels_dir, drop=False):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        str to csv with train features and labels\n",
    "        drop: to drop NaNs from feature columns.\n",
    "\n",
    "    Output:\n",
    "         X, y\n",
    "    \n",
    "    \"\"\"\n",
    "    features = pd.read_csv(features_dir)\n",
    "    labels = pd.read_csv(labels_dir)\n",
    "\n",
    "    data = features.merge(labels)\n",
    "\n",
    "    print(\"Preprocessing of training data started.\")\n",
    "    \n",
    "    if drop:\n",
    "        data_clean = data.dropna()\n",
    "        print(\"Droped {:.2f}% of rows.\".format((1 - data_clean.shape[0]/data.shape[0])*100))\n",
    "    else:\n",
    "        data_clean = data.copy()\n",
    "\n",
    "    #data_clean.loc[:,'log_total_cases'] = np.log1p(data_clean['total_cases'])\n",
    "    data_clean.loc[:,'log_total_cases'] = data_clean['total_cases']\n",
    "    data_clean_dt = get_dt_col(data_clean)\n",
    "\n",
    "    y = data_clean_dt['log_total_cases'].astype(int)\n",
    "    X = data_clean_dt.drop(columns=['total_cases','log_total_cases'])\n",
    "    print(\"Preprocessing of training data finished.\\n\")\n",
    "        \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bddad0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_features(X, y, corr_threshold = 0.3, split = False, skip = False):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        X\n",
    "        y\n",
    "        corr_threshold = 0.3\n",
    "        split: to split the data by cities\n",
    "\n",
    "    Output:\n",
    "        numerical_list, categorical_list, other_list\n",
    "    \"\"\"\n",
    "    data = X.copy()\n",
    "    data.loc[:,'log_total_cases'] = y\n",
    "\n",
    "    print(\"Feature selection in progress.\")\n",
    "    num_feat = data.select_dtypes(include=[np.number])\n",
    "\n",
    "    correlation_matrix = num_feat.corr()\n",
    "\n",
    "    cm = np.abs(correlation_matrix[['log_total_cases']]).sort_values(['log_total_cases'], ascending = False)\n",
    "    mask = cm>corr_threshold\n",
    "    boundary_index = np.sum(mask).iloc[0]\n",
    "\n",
    "    ordered_num_features = cm.index[1:].to_list()\n",
    "    numerical_selected = ordered_num_features[0:boundary_index-1]\n",
    "\n",
    "    if skip:\n",
    "        numerical_selected = num_feat.columns.to_list()\n",
    "        numerical_selected.remove('log_total_cases')\n",
    "        numerical_selected.pop(0)\n",
    "    \n",
    "    if split:\n",
    "        categorical_cols = []\n",
    "    else:\n",
    "        categorical_cols = ['city']\n",
    "        \n",
    "    other_cols = []\n",
    "    \n",
    "    # Ouput logs\n",
    "    print(\"Used {:.2f}% of numerical feature with maximum correlation = {:.4f} and minimum correlation= {:.4f}.\"\n",
    "          .format((1 - len(numerical_selected)/num_feat.shape[1])*100, \n",
    "          cm['log_total_cases'].iloc[1], cm['log_total_cases'].iloc[boundary_index-1]))\n",
    "    print(\"The categorical features are: {}\".format(categorical_cols))\n",
    "    print(\"The other selected features are: {}.\".format(other_cols))\n",
    "    print(\"Feature selection has been accomplished. \\n\")\n",
    "\n",
    "    return numerical_selected, categorical_cols, other_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76fb4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_pipeline(X, y, columns, model, train_size=0.8, validate = True, shuffle = True):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        X, y\n",
    "        corr_threshold: Feature to Y correlation threshold for feature selection.\n",
    "\n",
    "    Output:\n",
    "        print statement with val mean abs error\n",
    "        pipeline\n",
    "        # X_valid, y_valid (for later validation trials) - commented out for now\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=1-train_size, shuffle=shuffle)\n",
    "\n",
    "    numerical_cols, categorical_cols, other_cols = columns[0], columns[1], columns[2]\n",
    "\n",
    "    my_cols = categorical_cols + numerical_cols + other_cols\n",
    "\n",
    "    X_train = X_train_full[my_cols].copy()\n",
    "    X_valid = X_valid_full[my_cols].copy()\n",
    "\n",
    "    my_pipeline = get_pipeline(model, columns)\n",
    "\n",
    "    \n",
    "    if validate:\n",
    "        print(\"Training in progress.\")\n",
    "        my_pipeline.fit(X_train, y_train)\n",
    "        print(\"Training finished.\\n\")\n",
    "        print(\"Validation error in progress.\")\n",
    "        preds = my_pipeline.predict(X_valid)\n",
    "        score = mean_absolute_error(y_valid, preds)\n",
    "        print(' Finished Validation')\n",
    "        print('MAE:', score)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print(\"Training in progress.\")\n",
    "        my_pipeline.fit(X[my_cols], y)\n",
    "        print(\"Training finished.\\n\")\n",
    "\n",
    "    return my_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6456fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(model, columns):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        model\n",
    "        \n",
    "    Output:\n",
    "        pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    numerical_cols, categorical_cols = columns[0], columns[1]\n",
    "\n",
    "    numerical_transformer = Pipeline( steps =[\n",
    "        ('imputer', KNNImputer()),\n",
    "        ('scalar', StandardScaler()),\n",
    "        ('normalizer', Normalizer())\n",
    "        # ('MinMaxScaler')\n",
    "    ])\n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "    \n",
    "    return my_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9333e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(X_test, trained_pipeline, exp=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X_test\n",
    "        trained_pipeline\n",
    "\n",
    "    Output:\n",
    "        prediction vector\n",
    "\n",
    "        X_test -> apply dt_col\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = get_dt_col(X_test)\n",
    "    prediction = trained_pipeline.predict(X)\n",
    "\n",
    "    if exp:\n",
    "        return np.expm1(prediction)\n",
    "    else:\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be85624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_output(X_test, predictions, indexes = ['city', 'year', 'weekofyear'], exponentiate=False, file_dir = '../data/output.csv'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X_test - \n",
    "        predictions - for both cities indexed as X_test requires it and \n",
    "                        with the same number of rows as X_test. \n",
    "        indexes (List(str)) - By default the columns of X_test for the required format,\n",
    "                        i.e., ['city', 'year', 'weekofyear'];\n",
    "        exp (bool) - to exponentiate predictions or not;\n",
    "                     \n",
    "        file_dir (str) - directory with the filename of the output csv file;\n",
    "                         Give an empty string if saving the output is not desired.\n",
    "        \n",
    "    Returns:\n",
    "        output_df (pd.DataFrame) - Dataframe with formatted results\n",
    "        csv file\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Exponentiate the predictions if needed.\n",
    "    if exponentiate:\n",
    "        result = np.round(np.expm1(predictions)).astype(int)\n",
    "    else:\n",
    "        result = np.round(predictions).astype(int)\n",
    "\n",
    "    # 2. Make the dictionary for the output dataframe format.\n",
    "    out_dict = {}\n",
    "    for index in indexes:\n",
    "        out_dict[index] = X_test[index]\n",
    "\n",
    "    out_dict['total_cases'] = result\n",
    "    output_df = pd.DataFrame.from_dict(out_dict).set_index(indexes)\n",
    "    \n",
    "    # 3. Save predictions in a csv file ready for submission.\n",
    "    if file_dir:\n",
    "        print(\"Saved predictions in competition file format in path {} . /n\".format(file_dir))\n",
    "        output_df.to_csv(file_dir)\n",
    "        print('\\n')\n",
    "\n",
    "    return output_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d6f188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_city(from_dir = '../data/01_raw/', to_dir = '../data/02_intermediate/'):\n",
    "    \n",
    "    features = pd.read_csv(from_dir + 'dengue_features_train.csv')\n",
    "    labels = pd.read_csv(from_dir + 'dengue_labels_train.csv')\n",
    "    test = pd.read_csv(from_dir + 'dengue_features_test.csv')\n",
    "\n",
    "    features[features.city == 'sj'].to_csv(to_dir + 'dengue_features_train_sj.csv')\n",
    "    features[features.city == 'iq'].to_csv(to_dir + 'dengue_features_train_iq.csv')\n",
    "\n",
    "    labels[labels.city == 'sj'].to_csv(to_dir + 'dengue_labels_train_sj.csv')\n",
    "    labels[labels.city == 'iq'].to_csv(to_dir + 'dengue_labels_train_iq.csv')\n",
    "\n",
    "    test[test.city == 'sj'].to_csv(to_dir + 'dengue_features_test_sj.csv')\n",
    "    test[test.city == 'iq'].to_csv(to_dir + 'dengue_features_test_iq.csv')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5200b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_city_outputs(raw_dir = '../data/01_raw/', output_dir = '../data/07_model_output/'):\n",
    "\n",
    "    test_data = pd.read_csv(raw_dir + 'dengue_features_test.csv')\n",
    "\n",
    "    output_sj = pd.read_csv(output_dir + 'output_sj.csv')\n",
    "    output_iq = pd.read_csv(output_dir + 'output_iq.csv')\n",
    "\n",
    "    output = pd.concat([output_sj, output_iq])\n",
    "\n",
    "    test_data[['city','year','weekofyear']].merge(output).to_csv(output_dir + 'output_two_cities.csv', index=False)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c99a7",
   "metadata": {},
   "source": [
    "## 3. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7bccff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_threshold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ef7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_city()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4e214d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of training data started.\n",
      "Preprocessing of training data finished.\n",
      "\n",
      "Feature selection in progress.\n",
      "Used 8.33% of numerical feature with maximum correlation = 0.2871 and minimum correlation= 0.0003.\n",
      "The categorical features are: []\n",
      "The other selected features are: [].\n",
      "Feature selection has been accomplished. \n",
      "\n",
      "(['year', 'weekofyear', 'ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw', 'precipitation_amt_mm', 'reanalysis_air_temp_k', 'reanalysis_avg_temp_k', 'reanalysis_dew_point_temp_k', 'reanalysis_max_air_temp_k', 'reanalysis_min_air_temp_k', 'reanalysis_precip_amt_kg_per_m2', 'reanalysis_relative_humidity_percent', 'reanalysis_sat_precip_amt_mm', 'reanalysis_specific_humidity_g_per_kg', 'reanalysis_tdtr_k', 'station_avg_temp_c', 'station_diur_temp_rng_c', 'station_max_temp_c', 'station_min_temp_c', 'station_precip_mm'], [], [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/anaconda3/envs/DengAI/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "features_dir = '../data/02_intermediate/dengue_features_train_sj.csv'\n",
    "labels_dir = '../data/02_intermediate/dengue_labels_train_sj.csv'\n",
    "test_dir = '../data/02_intermediate/dengue_features_test_sj.csv'\n",
    "test_output_dir = '../data/07_model_output/output_sj.csv'\n",
    "\n",
    "X, y = get_train_data(features_dir=features_dir, labels_dir=labels_dir)\n",
    "columns = get_train_features(X=X, y=y, corr_threshold = corr_threshold, split=True, skip = True)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "785d942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of training data started.\n",
      "Preprocessing of training data finished.\n",
      "\n",
      "Feature selection in progress.\n",
      "Used 8.33% of numerical feature with maximum correlation = 0.2871 and minimum correlation= 0.0003.\n",
      "The categorical features are: []\n",
      "The other selected features are: [].\n",
      "Feature selection has been accomplished. \n",
      "\n",
      "Training in progress.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/anaconda3/envs/DengAI/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n",
      "Saved predictions in competition file format in path ../data/07_model_output/output_sj.csv . /n\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_dir = '../data/02_intermediate/dengue_features_train_sj.csv'\n",
    "labels_dir = '../data/02_intermediate/dengue_labels_train_sj.csv'\n",
    "test_dir = '../data/02_intermediate/dengue_features_test_sj.csv'\n",
    "test_output_dir = '../data/07_model_output/output_sj.csv'\n",
    "\n",
    "X, y = get_train_data(features_dir=features_dir, labels_dir=labels_dir)\n",
    "columns = get_train_features(X=X, y=y, corr_threshold = corr_threshold, split=True, skip = True)\n",
    "model = RandomForestRegressor(criterion='absolute_error')\n",
    "\n",
    "trained_pipeline = get_trained_pipeline(X=X, y=y, columns=columns, model=model, validate=False)\n",
    "\n",
    "\n",
    "X_test = pd.read_csv(test_dir)\n",
    "test_prediction = get_prediction(X_test, trained_pipeline, exp=False)\n",
    "result_df = get_test_output(X_test=X_test,\n",
    "                            predictions=test_prediction,\n",
    "                            exponentiate=False,\n",
    "                            file_dir=test_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf410920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of training data started.\n",
      "Preprocessing of training data finished.\n",
      "\n",
      "Feature selection in progress.\n",
      "Used 8.33% of numerical feature with maximum correlation = 0.2365 and minimum correlation= 0.0096.\n",
      "The categorical features are: []\n",
      "The other selected features are: [].\n",
      "Feature selection has been accomplished. \n",
      "\n",
      "Training in progress.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/anaconda3/envs/DengAI/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n",
      "Saved predictions in competition file format in path ../data/07_model_output/output_iq.csv . /n\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_dir = '../data/02_intermediate/dengue_features_train_iq.csv'\n",
    "labels_dir = '../data/02_intermediate/dengue_labels_train_iq.csv'\n",
    "test_dir = '../data/02_intermediate/dengue_features_test_iq.csv'\n",
    "test_output_dir = '../data/07_model_output/output_iq.csv'\n",
    "\n",
    "X, y = get_train_data(features_dir=features_dir, labels_dir=labels_dir)\n",
    "columns = get_train_features(X=X, y=y, corr_threshold = corr_threshold, split=True, skip = True)\n",
    "model = RandomForestRegressor(criterion='absolute_error')\n",
    "\n",
    "trained_pipeline = get_trained_pipeline(X=X, y=y, columns=columns, model=model, validate=False)\n",
    "\n",
    "\n",
    "X_test = pd.read_csv(test_dir)\n",
    "test_prediction = get_prediction(X_test, trained_pipeline, exp=False)\n",
    "result_df = get_test_output(X_test=X_test,\n",
    "                            predictions=test_prediction,\n",
    "                            exponentiate=False,\n",
    "                            file_dir=test_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ed50c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_city_outputs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DengAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc82d37",
   "metadata": {},
   "source": [
    "# 1.Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fb5bed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c3fa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           CalendarSeasonality,\n",
    "                                           CalendarTimeTrend,\n",
    "                                           DeterministicProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71942d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57819988",
   "metadata": {},
   "source": [
    "# 2.Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631cb1e",
   "metadata": {},
   "source": [
    "### Loading data and splitting X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69948f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(features_dir, labels_dir):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        str to csv with train features and labels\n",
    "        drop: to drop NaNs from feature columns.\n",
    "\n",
    "    Output:\n",
    "         X, y\n",
    "    \n",
    "    \"\"\"\n",
    "    features = pd.read_csv(features_dir)\n",
    "    labels = pd.read_csv(labels_dir)\n",
    "\n",
    "    data = features.merge(labels)\n",
    "    \n",
    "    X = data.drop(columns=['total_cases'])\n",
    "    y = data.loc[:,'total_cases']\n",
    "    print(\"Loading of training data finished.\\n\")\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa0a0a",
   "metadata": {},
   "source": [
    "### Building fetures from 'week_start_date' (month, day of year, week of year):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f6d078f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_features(X, time_column='week_start_date', format='%Y-%m-%d'):\n",
    "    \n",
    "    # Timed features.\n",
    "    X_new = X.copy()\n",
    "\n",
    "    X_new[time_column] = pd.to_datetime(X_new[time_column], format=format)\n",
    "    X_new['year'] =  X_new[time_column].apply(lambda x: x.year)\n",
    "    X_new['month'] = X_new[time_column].apply(lambda x: x.month)\n",
    "    X_new['dayofyear'] = X_new[time_column].apply(lambda x: x.dayofyear)\n",
    "    X_new['weekofyear'] =  X_new[time_column].apply(lambda x: x.weekofyear)\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a3137",
   "metadata": {},
   "source": [
    "### Building lagged features based on all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "edd16e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lags(X, columns, lags=8):\n",
    "    new_dict = {}\n",
    "    labels = []\n",
    "    for column in columns:\n",
    "        for lag in range(1,lags+1):\n",
    "            #X_new.loc[:, column+'_lag_{}'.format(lag)] = X[column].shift(lag)\n",
    "            new_dict[column+'_lag_{}'.format(lag)]= X[column].shift(lag)\n",
    "            labels.append(column+'_lag_{}'.format(lag))\n",
    "\n",
    "    X_new = pd.DataFrame(new_dict, columns=labels, index=X.index)\n",
    "    return pd.concat([X, X_new], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb96a61",
   "metadata": {},
   "source": [
    "### Building seasonality features (sin & cos based on timestamps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6b8bb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seasonality(X, time_column='week_start_date', format='%Y-%m-%d'):\n",
    "    X_new = X.copy()\n",
    "    X_new[time_column] = pd.to_datetime(X_new[time_column], format=format)\n",
    " \n",
    "    #return_cols = [time_column, 'year', 'month', 'dayofyear', 'weekofyear']\n",
    "    #X_fourier = X_new[time_column]\n",
    "    \n",
    "    fourier_year = CalendarFourier(freq='A', order=5)\n",
    "    fourier_month = CalendarFourier(freq='M', order=6)\n",
    "    fourier_week = CalendarFourier(freq='W', order=10)\n",
    "    fourier_day = CalendarFourier(freq='D', order=3)\n",
    "    df_year = fourier_year.in_sample(X_new[time_column]).set_index(X_new.index)\n",
    "    df_month = fourier_month.in_sample(X_new[time_column]).set_index(X_new.index)\n",
    "    df_week = fourier_week.in_sample(X_new[time_column]).set_index(X_new.index)\n",
    "    df_day = fourier_day.in_sample(X_new[time_column]).set_index(X_new.index)\n",
    "\n",
    "    ym = pd.concat([df_year,df_month], axis=1)\n",
    "    wd = pd.concat([df_week,df_day], axis=1)\n",
    "    ymwd = pd.concat([ym, wd], axis=1)\n",
    "\n",
    "    return pd.concat([X_new, ymwd], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b747bd7",
   "metadata": {},
   "source": [
    "### Building a simpleImputer() from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6456fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputer(X):\n",
    "\n",
    "    numerical_cols = list(X.select_dtypes(include=[np.number]).columns)\n",
    "\n",
    "    numerical_transformer = Pipeline( steps =[\n",
    "        ('imputer', SimpleImputer())\n",
    "    ])\n",
    "\n",
    "    #categorical_transformer = Pipeline( steps =[('donothing', 'passthrough')])\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols)\n",
    "        ])\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da402e8",
   "metadata": {},
   "source": [
    "### Formatting output for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1be85624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_output(X_test, predictions, indexes = ['city', 'year', 'weekofyear'], exponentiate=False, file_dir = '../data/output.csv'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X_test - \n",
    "        predictions - for both cities indexed as X_test requires it and \n",
    "                        with the same number of rows as X_test. \n",
    "        indexes (List(str)) - By default the columns of X_test for the required format,\n",
    "                        i.e., ['city', 'year', 'weekofyear'];\n",
    "        exp (bool) - to exponentiate predictions or not;\n",
    "                     \n",
    "        file_dir (str) - directory with the filename of the output csv file;\n",
    "                         Give an empty string if saving the output is not desired.\n",
    "        \n",
    "    Returns:\n",
    "        output_df (pd.DataFrame) - Dataframe with formatted results\n",
    "        csv file\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Exponentiate the predictions if needed.\n",
    "    if exponentiate:\n",
    "        result = np.round(np.expm1(predictions)).astype(int)\n",
    "    else:\n",
    "        result = np.round(predictions).astype(int)\n",
    "\n",
    "    # 2. Make the dictionary for the output dataframe format.\n",
    "    out_dict = {}\n",
    "    for index in indexes:\n",
    "        out_dict[index] = X_test[index]\n",
    "\n",
    "    out_dict['total_cases'] = result\n",
    "    output_df = pd.DataFrame.from_dict(out_dict).set_index(indexes)\n",
    "    \n",
    "    # 3. Save predictions in a csv file ready for submission.\n",
    "    if file_dir:\n",
    "        print(\"Saved predictions in competition file format in path {} . /n\".format(file_dir))\n",
    "        output_df.to_csv(file_dir)\n",
    "        print('\\n')\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5eea6",
   "metadata": {},
   "source": [
    "### Splitting train & test data by city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4d6f188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_city(from_dir = '../data/01_raw/', to_dir = '../data/02_intermediate/'):\n",
    "    \n",
    "    features = pd.read_csv(from_dir + 'dengue_features_train.csv')\n",
    "    labels = pd.read_csv(from_dir + 'dengue_labels_train.csv')\n",
    "    test = pd.read_csv(from_dir + 'dengue_features_test.csv')\n",
    "\n",
    "    features[features.city == 'sj'].to_csv(to_dir + 'dengue_features_train_sj.csv', index=False)\n",
    "    features[features.city == 'iq'].to_csv(to_dir + 'dengue_features_train_iq.csv', index=False)\n",
    "\n",
    "    labels[labels.city == 'sj'].to_csv(to_dir + 'dengue_labels_train_sj.csv', index=False)\n",
    "    labels[labels.city == 'iq'].to_csv(to_dir + 'dengue_labels_train_iq.csv', index=False)\n",
    "\n",
    "    test[test.city == 'sj'].to_csv(to_dir + 'dengue_features_test_sj.csv', index=False)\n",
    "    test[test.city == 'iq'].to_csv(to_dir + 'dengue_features_test_iq.csv', index=False)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41c765",
   "metadata": {},
   "source": [
    "### Binding city outputs together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5200b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_city_outputs(raw_dir = '../data/01_raw/', output_dir = '../data/07_model_output/'):\n",
    "\n",
    "    test_data = pd.read_csv(raw_dir + 'dengue_features_test.csv')\n",
    "\n",
    "    output_sj = pd.read_csv(output_dir + 'output_sj.csv')\n",
    "    output_iq = pd.read_csv(output_dir + 'output_iq.csv')\n",
    "\n",
    "    output = pd.concat([output_sj, output_iq])\n",
    "\n",
    "    test_data[['city','year','weekofyear']].merge(output).to_csv(output_dir + 'output_two_cities.csv', index=False)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c99a7",
   "metadata": {},
   "source": [
    "## 3. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c85ef7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_cols = ['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw',\n",
    "       'precipitation_amt_mm', 'reanalysis_air_temp_k',\n",
    "       'reanalysis_avg_temp_k', 'reanalysis_dew_point_temp_k',\n",
    "       'reanalysis_max_air_temp_k', 'reanalysis_min_air_temp_k',\n",
    "       'reanalysis_precip_amt_kg_per_m2',\n",
    "       'reanalysis_relative_humidity_percent', 'reanalysis_sat_precip_amt_mm',\n",
    "       'reanalysis_specific_humidity_g_per_kg', 'reanalysis_tdtr_k',\n",
    "       'station_avg_temp_c', 'station_diur_temp_rng_c', 'station_max_temp_c',\n",
    "       'station_min_temp_c', 'station_precip_mm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1399a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_city()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = '../data/02_intermediate/dengue_features_train_sj.csv'\n",
    "labels_dir = '../data/02_intermediate/dengue_labels_train_sj.csv'\n",
    "test_dir = '../data/02_intermediate/dengue_features_test_sj.csv'\n",
    "test_output_dir = '../data/07_model_output/output_sj.csv'\n",
    "\n",
    "X, y = get_train_data(features_dir=features_dir, labels_dir=labels_dir)\n",
    "model = RandomForestRegressor(criterion='absolute_error', n_estimators=200)\n",
    "#model = xgb.XGBRegressor(max_depth=2, learning_rate=0.01, n_estimators=500, n_jobs=-1)\n",
    "\n",
    "# preprocessing & training\n",
    "X_train = get_time_features(X)\n",
    "X_train = make_lags(X_train, lag_cols, lags=8)\n",
    "X_train = get_seasonality(X_train, time_column='week_start_date', format='%Y-%m-%d')\n",
    "my_imputer = get_imputer(X_train)\n",
    "num_cols = list(X_train.select_dtypes(include=[np.number]).columns)\n",
    "X_train = my_imputer.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=num_cols)\n",
    "\n",
    "model.fit(X_train, y)\n",
    "\n",
    "# test preprocessing & predict\n",
    "X_test = pd.read_csv(test_dir)\n",
    "X_test_processed = get_time_features(X_test)\n",
    "X_test_processed = make_lags(X_test_processed, lag_cols, lags=8)\n",
    "X_test_processed = get_seasonality(X_test_processed, time_column='week_start_date', format='%Y-%m-%d')\n",
    "num_cols = list(X_test_processed.select_dtypes(include=[np.number]).columns)\n",
    "X_test_processed = my_imputer.transform(X_test_processed)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=num_cols)\n",
    "\n",
    "predictions = model.predict(X_test_processed)\n",
    "\n",
    "# format output\n",
    "get_test_output(X_test=X_test,\n",
    "                predictions=predictions,\n",
    "                file_dir=test_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf410920",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = '../data/02_intermediate/dengue_features_train_iq.csv'\n",
    "labels_dir = '../data/02_intermediate/dengue_labels_train_iq.csv'\n",
    "test_dir = '../data/02_intermediate/dengue_features_test_iq.csv'\n",
    "test_output_dir = '../data/07_model_output/output_iq.csv'\n",
    "\n",
    "X, y = get_train_data(features_dir=features_dir, labels_dir=labels_dir)\n",
    "model = RandomForestRegressor(criterion='absolute_error', n_estimators=200)\n",
    "#model = xgb.XGBRegressor(max_depth=2, learning_rate=0.01, n_estimators=500, n_jobs=-1)\n",
    "\n",
    "# preprocessing & training\n",
    "X_train = get_time_features(X)\n",
    "X_train = make_lags(X_train, lag_cols, lags=8)\n",
    "X_train = get_seasonality(X_train, time_column='week_start_date', format='%Y-%m-%d')\n",
    "my_imputer = get_imputer(X_train)\n",
    "num_cols = list(X_train.select_dtypes(include=[np.number]).columns)\n",
    "X_train = my_imputer.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=num_cols)\n",
    "\n",
    "model.fit(X_train, y)\n",
    "\n",
    "# test preprocessing & predict\n",
    "X_test = pd.read_csv(test_dir)\n",
    "X_test_processed = get_time_features(X_test)\n",
    "X_test_processed = make_lags(X_test_processed, lag_cols, lags=8)\n",
    "X_test_processed = get_seasonality(X_test_processed, time_column='week_start_date', format='%Y-%m-%d')\n",
    "num_cols = list(X_test_processed.select_dtypes(include=[np.number]).columns)\n",
    "X_test_processed = my_imputer.transform(X_test_processed)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=num_cols)\n",
    "\n",
    "predictions = model.predict(X_test_processed)\n",
    "\n",
    "# format output\n",
    "get_test_output(X_test=X_test,\n",
    "                predictions=predictions,\n",
    "                file_dir=test_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ed50c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_city_outputs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DengAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2108fe6f-2fcb-40da-ae71-4d2013cfb0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5bed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57819988",
   "metadata": {},
   "source": [
    "## 1. Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4909826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = '../data/01_raw/dengue_features_train.csv'\n",
    "labels_dir = '../data/01_raw/dengue_labels_train.csv'\n",
    "test_dir = '../data/01_raw/dengue_features_test.csv'\n",
    "\n",
    "test_output_dir = 'output.csv'\n",
    "\n",
    "corr_threshold = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631cb1e",
   "metadata": {},
   "source": [
    "## 2. Building blocks functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1997641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt_col(X , column='week_start_date', format='%Y-%m-%d'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X\n",
    "        col = weekofx\n",
    "\n",
    "    Returns:\n",
    "        X with X[col].dtype = datetime\n",
    "\n",
    "    \"\"\"\n",
    "    X_new = X.copy()\n",
    "    X_new[column] = pd.to_datetime(X_new[column], format=format)\n",
    "\n",
    "    return X_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69948f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(features_dir, labels_dir, drop=False):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        str to csv with train features and labels\n",
    "        drop: to drop NaNs from feature columns.\n",
    "\n",
    "    Output:\n",
    "         X, y\n",
    "    \n",
    "    \"\"\"\n",
    "    features = pd.read_csv(features_dir)\n",
    "    labels = pd.read_csv(labels_dir)\n",
    "\n",
    "    data = features.merge(labels)\n",
    "\n",
    "    print(\"Preprocessing of training data started.\")\n",
    "    \n",
    "    if drop:\n",
    "        data_clean = data.dropna()\n",
    "        print(\"Droped {:.2f}% of rows.\".format((1 - data_clean.shape[0]/data.shape[0])*100))\n",
    "    else:\n",
    "        data_clean = data.copy()\n",
    "\n",
    "    data_clean.loc[:,'log_total_cases'] = np.log1p(data_clean['total_cases'])\n",
    "    data_clean_dt = get_dt_col(data_clean)\n",
    "\n",
    "    y = data_clean_dt['log_total_cases']\n",
    "    X = data_clean_dt.drop(columns=['total_cases','log_total_cases'])\n",
    "    print(\"Preprocessing of training data finished.\\n\")\n",
    "        \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bddad0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_features(X, y, corr_threshold = 0.3, split = False):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        X\n",
    "        y\n",
    "        corr_threshold = 0.3\n",
    "        split: to split the data by cities\n",
    "\n",
    "    Output:\n",
    "        numerical_list, categorical_list, other_list\n",
    "    \"\"\"\n",
    "    data = X.copy()\n",
    "    data.loc[:,'log_total_cases'] = y\n",
    "\n",
    "    print(\"Feature selection in progress.\")\n",
    "    num_feat = data.select_dtypes(include=[np.number])\n",
    "\n",
    "    correlation_matrix = num_feat.corr()\n",
    "\n",
    "    cm = np.abs(correlation_matrix[['log_total_cases']]).sort_values(['log_total_cases'], ascending = False)\n",
    "    mask = cm>corr_threshold\n",
    "    boundary_index = np.sum(mask).iloc[0]\n",
    "\n",
    "    ordered_num_features = cm.index[1:].to_list()\n",
    "    numerical_selected = ordered_num_features[0:boundary_index-1]\n",
    "    \n",
    "    if split:\n",
    "        categorical_cols = []\n",
    "    else:\n",
    "        categorical_cols = ['city']\n",
    "        \n",
    "    other_cols = ['week_start_date']\n",
    "    \n",
    "    # Ouput logs\n",
    "    print(\"Used {:.2f}% of numerical feature with maximum correlation = {:.4f} and minimum correlation= {:.4f}.\"\n",
    "          .format((1 - len(numerical_selected)/num_feat.shape[1])*100, \n",
    "          cm['log_total_cases'].iloc[1], cm['log_total_cases'].iloc[boundary_index-1]))\n",
    "    print(\"The categorical features are: {}\".format(categorical_cols))\n",
    "    print(\"The other selected features are: {}.\".format(other_cols))\n",
    "    print(\"Feature selection has been accomplished. \\n\")\n",
    "\n",
    "    return numerical_selected, categorical_cols, other_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76fb4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_pipeline(X, y, columns, model, train_size=0.8, validate = True, shuffle = True):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        X, y\n",
    "        corr_threshold: Feature to Y correlation threshold for feature selection.\n",
    "\n",
    "    Output:\n",
    "        print statement with val mean abs error\n",
    "        pipeline\n",
    "        # X_valid, y_valid (for later validation trials) - commented out for now\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=1-train_size, shuffle=shuffle)\n",
    "\n",
    "    numerical_cols, categorical_cols, other_cols = columns[0], columns[1], columns[2]\n",
    "\n",
    "    my_cols = categorical_cols + numerical_cols + other_cols\n",
    "\n",
    "    X_train = X_train_full[my_cols].copy()\n",
    "    X_valid = X_valid_full[my_cols].copy()\n",
    "\n",
    "    my_pipeline = get_pipeline(model, columns)\n",
    "    print(\"Training in progress.\")\n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "    print(\"Training finished.\\n\")\n",
    "    \n",
    "    if validate:\n",
    "        print(\"Validation error in progress.\")\n",
    "        preds = my_pipeline.predict(X_valid)\n",
    "        score = mean_absolute_error(y_valid, preds)\n",
    "        print(' Finished Validation')\n",
    "        print('MAE:', score)\n",
    "        print('\\n')\n",
    "\n",
    "    return my_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6456fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(model, columns):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        model\n",
    "        \n",
    "    Output:\n",
    "        pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    numerical_cols, categorical_cols = columns[0], columns[1]\n",
    "\n",
    "    numerical_transformer = Pipeline( steps =[\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('scalar', StandardScaler()),\n",
    "        ('normalizer', Normalizer())\n",
    "        # ('MinMaxScaler')\n",
    "    ])\n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "    \n",
    "    return my_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9333e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(X_test, trained_pipeline, exp=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X_test\n",
    "        trained_pipeline\n",
    "\n",
    "    Output:\n",
    "        prediction vector\n",
    "\n",
    "        X_test -> apply dt_col\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = get_dt_col(X_test)\n",
    "    prediction = trained_pipeline.predict(X)\n",
    "\n",
    "    if exp:\n",
    "        return np.expm1(prediction)\n",
    "    else:\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1be85624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_output(X_test, predictions, indexes = ['city', 'year', 'weekofyear'], exponentiate=False, file_dir = '../data/output.csv'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X_test - \n",
    "        predictions - for both cities indexed as X_test requires it and \n",
    "                        with the same number of rows as X_test. \n",
    "        indexes (List(str)) - By default the columns of X_test for the required format,\n",
    "                        i.e., ['city', 'year', 'weekofyear'];\n",
    "        exp (bool) - to exponentiate predictions or not;\n",
    "                     \n",
    "        file_dir (str) - directory with the filename of the output csv file;\n",
    "                         Give an empty string if saving the output is not desired.\n",
    "        \n",
    "    Returns:\n",
    "        output_df (pd.DataFrame) - Dataframe with formatted results\n",
    "        csv file\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Exponentiate the predictions if needed.\n",
    "    if exponentiate:\n",
    "        result = np.round(np.expm1(predictions)).astype(int)\n",
    "    else:\n",
    "        result = np.round(predictions).astype(int)\n",
    "\n",
    "    # 2. Make the dictionary for the output dataframe format.\n",
    "    out_dict = {}\n",
    "    for index in indexes:\n",
    "        out_dict[index] = X_test[index]\n",
    "\n",
    "    out_dict['total_cases'] = result\n",
    "    output_df = pd.DataFrame.from_dict(out_dict).set_index(indexes)\n",
    "    \n",
    "    # 3. Save predictions in a csv file ready for submission.\n",
    "    if file_dir:\n",
    "        print(\"Saved predictions in competition file format in path {} . /n\".format(file_dir))\n",
    "        output_df.to_csv(file_dir)\n",
    "        print('\\n')\n",
    "\n",
    "    return output_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f188d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "028c99a7",
   "metadata": {},
   "source": [
    "## 3. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7bccff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of training data started.\n",
      "Preprocessing of training data finished.\n",
      "\n",
      "Feature selection in progress.\n",
      "Used 56.52% of numerical feature with maximum correlation = 0.5640 and minimum correlation= 0.3234.\n",
      "The categorical features are: ['city']\n",
      "The other selected features are: ['week_start_date'].\n",
      "Feature selection has been accomplished. \n",
      "\n",
      "Training in progress.\n",
      "Training finished.\n",
      "\n",
      "Validation error in progress.\n",
      " Finished Validation\n",
      "MAE: 0.730682949005616\n",
      "\n",
      "\n",
      "Saved predictions in competition file format in path output.csv . /n\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/miniconda3/envs/dsr-project/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, y = get_train_data(features_dir=features_dir, labels_dir=labels_dir)\n",
    "columns = get_train_features(X=X, y=y, corr_threshold = corr_threshold)\n",
    "model = LinearRegression()\n",
    "\n",
    "trained_pipeline = get_trained_pipeline(X=X, y=y, columns=columns, model=model)\n",
    "\n",
    "\n",
    "X_test = pd.read_csv(test_dir)\n",
    "test_prediction = get_prediction(X_test, trained_pipeline, exp=False)\n",
    "result_df = get_test_output(X_test=X_test,\n",
    "                            predictions=test_prediction,\n",
    "                            exponentiate=True,\n",
    "                            file_dir=test_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50c82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebb7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3fbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsr-project-ker",
   "language": "python",
   "name": "dsr-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
